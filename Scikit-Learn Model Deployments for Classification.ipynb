{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Model Deployments for SVC, RF, Boosting Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "from Crypto.Cipher import AES\n",
    "from sklearn.tree import _tree\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Decision-Tree as List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_list(tree, feature_names, num):\n",
    "    code_list = ''\n",
    "    \n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    #code_list = '[input feature, node, [input feature, node, left leaf, right leaf], right leaf]'\n",
    "    def recurse(node, depth, code_list):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            code_list = code_list + '[\"{}\",{},'.format(name, threshold)\n",
    "            code_list = recurse(tree_.children_left[node], depth + 1, code_list)\n",
    "            code_list = code_list + \",\"\n",
    "            code_list = recurse(tree_.children_right[node], depth + 1, code_list)\n",
    "            code_list = code_list + \"]\"\n",
    "        else:\n",
    "            #print(tree_.value[node])\n",
    "            #proba = list(tree_.value[node][0]/sum(tree_.value[node][0]))\n",
    "            proba = list(tree_.value[node][0])\n",
    "            code_list = code_list + \"{}\".format(proba)\n",
    "        \n",
    "        return code_list\n",
    "    code_list = recurse(0, 2, code_list)\n",
    "    \n",
    "    return code_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Decision-Tree in List format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict_list(x_input_l, lst):\n",
    "    \"\"\"\n",
    "    Function to get the prediction value where trees are stored in list format recursively\n",
    "    Format: [input feature (column index), node value, left leaf (yes), right leaf (no)]\n",
    "    Input: x_input_l (input for obaining the tree prediction), lst (tree in list format)\n",
    "    Output: pred (final pred value for the provided input)\n",
    "    \"\"\"\n",
    "    if isinstance(lst, (float, int)):\n",
    "        return lst\n",
    "    \n",
    "    if isinstance(lst, list) and len(lst) == 1:\n",
    "        return lst\n",
    "\n",
    "    if isinstance(lst[2], list) and len(lst[2]) == 4 and isinstance(lst[2][0], str):\n",
    "        left_leaf = tree_predict_list(x_input_l, lst[2])\n",
    "    else:\n",
    "        left_leaf = lst[2]\n",
    "    if isinstance(lst[3], list) and len(lst[3]) == 4 and isinstance(lst[3][0], str):\n",
    "        right_leaf = tree_predict_list(x_input_l, lst[3])\n",
    "    else:\n",
    "        right_leaf = lst[3]\n",
    "\n",
    "    # If input feature <= node value then left leaf else right leaf\n",
    "    pred = left_leaf if x_input_l[int(lst[0])] <= lst[1] else right_leaf\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_decision_function(k, nv, a, b, cs):\n",
    "    # Start and end index of each support vector\n",
    "    start = [sum(nv[:i]) for i in range(len(nv))]\n",
    "    end = [start[i] + nv[i] for i in range(len(nv))]\n",
    "\n",
    "    # Calculate: sum(a_p * k(x_p, x)) between every 2 classes\n",
    "    c = [ sum(a[ i ][p] * k[p] for p in range(start[j], end[j])) +\n",
    "          sum(a[j-1][q] * k[q] for q in range(start[i], end[i]))\n",
    "                for i in range(len(nv)) for j in range(i+1,len(nv))]\n",
    "\n",
    "    # Add the intercept term\n",
    "    decision = [c[i] + b[i] for i in range(len(b))]\n",
    "    decision = np.array([x for x in decision]).T\n",
    "    \n",
    "    # Obtain the vote based on the decision value\n",
    "    votes = [[(0 if decision[:, p] > 0 else 1) for p,(i,j) in enumerate((i,j) \n",
    "                                           for i in range(len(cs))\n",
    "                                           for j in range(i+1,len(cs)))]]\n",
    "    \n",
    "    # Obtain the One-vs-Rest Decision value\n",
    "    decision_ovr = ovr_decision_function(np.array(votes), -decision, len(cs))\n",
    "    return decision_ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-vs-One decision function into One-vs-Rest decision function \n",
    "# From Scikit-Learn source code\n",
    "def ovr_decision_function(predictions, confidences, n_classes):\n",
    "    \"\"\"Compute a continuous, tie-breaking OvR decision function from OvO.\n",
    "    It is important to include a continuous value, not only votes,\n",
    "    to make computing AUC or calibration meaningful.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : array-like, shape (n_samples, n_classifiers)\n",
    "        Predicted classes for each binary classifier.\n",
    "    confidences : array-like, shape (n_samples, n_classifiers)\n",
    "        Decision functions or predicted probabilities for positive class\n",
    "        for each binary classifier.\n",
    "    n_classes : int\n",
    "        Number of classes. n_classifiers must be\n",
    "        ``n_classes * (n_classes - 1 ) / 2``\n",
    "    \"\"\"\n",
    "    n_samples = predictions.shape[0]\n",
    "    votes = np.zeros((n_samples, n_classes))\n",
    "    sum_of_confidences = np.zeros((n_samples, n_classes))\n",
    "\n",
    "    k = 0\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            sum_of_confidences[:, i] -= confidences[:, k]\n",
    "            sum_of_confidences[:, j] += confidences[:, k]\n",
    "            votes[predictions[:, k] == 0, i] += 1\n",
    "            votes[predictions[:, k] == 1, j] += 1\n",
    "            k += 1\n",
    "\n",
    "    # Monotonically transform the sum_of_confidences to (-1/3, 1/3)\n",
    "    # and add it with votes. The monotonic transformation  is\n",
    "    # f: x -> x / (3 * (|x| + 1)), it uses 1/3 instead of 1/2\n",
    "    # to ensure that we won't reach the limits and change vote order.\n",
    "    # The motivation is to use confidence levels as a way to break ties in\n",
    "    # the votes without switching any decision made based on a difference\n",
    "    # of 1 vote.\n",
    "\n",
    "    transformed_confidences = (sum_of_confidences /\n",
    "                               (3 * (np.abs(sum_of_confidences) + 1)))\n",
    "\n",
    "    return votes + transformed_confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypt & Decrypt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_json(json_obj):\n",
    "    \"\"\"\n",
    "    Scikit-Learn Model in JSON format is converted into encrypted JSON.\n",
    "    Input: JSON object\n",
    "    Output: Encrypted JSON.\n",
    "    \"\"\"\n",
    "    secret_key = b'abcdefghijklmnop'    \n",
    "    iv = b'abcdefghijklmKEY'\n",
    "    \n",
    "    BS = 16\n",
    "    pad = lambda s: s + (BS - len(s) % BS) * chr(BS - len(s) % BS)    \n",
    "    cipher = AES.new(secret_key,AES.MODE_CBC,iv)\n",
    "    json_string = json.dumps(json_obj)\n",
    "    json_string = pad(json_string)\n",
    "    ENCODED= (cipher.encrypt(json_string.encode(\"utf8\")))\n",
    "    return ENCODED\n",
    "    \n",
    "def decrypt_json(model_name):\n",
    "    \"\"\"\n",
    "    Models saved as encrypted json files are loaded and decrypted\n",
    "    Input: model_name (name of model to be loaded from saved_models folder)\n",
    "    Output: decrpyted model in json/dictionary format\n",
    "    \"\"\"\n",
    "    folder_path = \"saved_models\\\\\"\n",
    "    filename = folder_path + model_name\n",
    "\n",
    "    unpad = lambda s: s[0:-s[-1]]\n",
    "\n",
    "    secret_key = b'abcdefghijklmnop'\n",
    "    init_vector = b'abcdefghijklmKEY'\n",
    "    with open(filename + '_json_enc.txt', 'rb') as op_file:\n",
    "        json_enc = op_file.read()\n",
    "    cipher = AES.new(secret_key, AES.MODE_CBC, init_vector)\n",
    "    decoded = cipher.decrypt(json_enc)\n",
    "    prog_block = unpad(decoded)\n",
    "    return json.loads(prog_block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save function: SVC/RF/Boosting model --> JSON --> Encrypted JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_json(folder_path, model_name, model_save, model_input, model_output, model_type):\n",
    "    model_par_len = model_input.shape[1]\n",
    "    model_X_mean = list(model_input.mean(axis = 0))\n",
    "    model_X_std = list(model_input.std(axis = 0))\n",
    "    model_X_var = ((model_input-model_input.mean(axis = 0))/model_input.std(axis = 0)).var()\n",
    "    \n",
    "    py_filename_json = folder_path + model_name + '_class.json'\n",
    "    py_filename_enc = folder_path + model_name + '_json_enc.txt'\n",
    "    \n",
    "    dict_file = {}\n",
    "    if model_type == 'SVC':\n",
    "        dict_file['model_type'] = model_type\n",
    "        \n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        value_counts = [[e1, counts[i]] for i, e1 in enumerate(values)]\n",
    "        dict_file['class_names'] = [str(x) for x in values]\n",
    "        \n",
    "        dict_file['kernel'] = model_save[1].kernel\n",
    "        dict_file['degree'] = model_save[1].degree\n",
    "        dict_file['gamma'] = model_save[1].gamma\n",
    "        dict_file['coef0'] = model_save[1].coef0\n",
    "        dict_file['epsilon'] = model_save[1].epsilon\n",
    "        dict_file['n_features'] = model_par_len\n",
    "        dict_file['input_variance'] = model_X_var\n",
    "        dict_file['intercept'] = model_save[1].intercept_.tolist()\n",
    "        dict_file['mean_value'] = model_X_mean\n",
    "        dict_file['std_value'] = model_X_std\n",
    "        dict_file['dual_coef'] = model_save[1].dual_coef_.tolist()\n",
    "        dict_file['support_vectors'] = model_save[1].support_vectors_.tolist()\n",
    "        dict_file['n_support'] = model_save[1].n_support_.tolist()\n",
    "    elif model_type == 'RF':\n",
    "        dict_file['model_type'] = model_type\n",
    "        \n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        value_counts = [[e1, counts[i]] for i, e1 in enumerate(values)]\n",
    "        dict_file['class_names'] = [str(x) for x in values]\n",
    "\n",
    "        for i in range(model_save.n_estimators):\n",
    "            tree_list = tree_to_list(model_save.estimators_[i], [str(j) for j in range(model_par_len)], i) \n",
    "            dict_file['tree_'+ str(i)] = json.loads(tree_list)\n",
    "    elif model_type == 'GradientBoost':\n",
    "        dict_file['model_type'] = model_type\n",
    "        dict_file['learning_rate'] = model_save.learning_rate\n",
    "        \n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        dict_file['class_names'] = [str(x) for x in values]\n",
    "        \n",
    "        if len(values) > 2:\n",
    "            dict_file['base_estimator_value'] = [np.log(x/sum(counts)) for x  in counts]\n",
    "        else:\n",
    "            value_counts = [[e1, counts[i]] for i, e1 in enumerate(values)]\n",
    "            value_list = [value_counts[i][1] for i, e1 in enumerate(value_counts)]\n",
    "            dict_file['base_estimator_value'] = [np.log(value_list[1]/value_list[0])]\n",
    "\n",
    "        for i in range(model_save.n_estimators):\n",
    "            tr_x = []\n",
    "            for k in range(len(model_save.estimators_[i])):\n",
    "                tree_list = tree_to_list(model_save.estimators_[i][k], [str(j) for j in range(model_par_len)], i) \n",
    "                tr_x.append(json.loads(tree_list))\n",
    "            dict_file['tree_'+ str(i)] = tr_x\n",
    "    elif model_type == 'AdaBoost':\n",
    "        dict_file['model_type'] = model_type\n",
    "        dict_file['algorithm'] = model_save.algorithm\n",
    "        dict_file['estimator_weights'] = model_save.estimator_weights_.tolist()\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        value_counts = [[e1, counts[i]] for i, e1 in enumerate(values)]\n",
    "        dict_file['class_names'] = [str(x) for x in values]\n",
    "        \n",
    "        for i in range(len(model_save.estimators_)):\n",
    "            tree_list = tree_to_list(model_save.estimators_[i], [str(j) for j in range(model_par_len)], i) \n",
    "            dict_file['tree_'+ str(i)] = json.loads(tree_list)\n",
    "            \n",
    "    with open(py_filename_json, \"w\") as write_file:\n",
    "        json.dump(dict_file, write_file)\n",
    "    \n",
    "    json_ciphertext = encrypt_json(dict_file)\n",
    "    \n",
    "    with open(py_filename_enc, \"wb\") as write_file:\n",
    "        write_file.write(json_ciphertext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function: SVR/RF/Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict_json(model, x_input):\n",
    "    '''\n",
    "    Machine Learning models predict function is recreated using the stored values\n",
    "    '''\n",
    "    output_list = []\n",
    "    class_names = model['class_names']\n",
    "    n_classes = len(class_names)\n",
    "    # SVC Model\n",
    "    if model['model_type'] == 'SVC':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - model['mean_value'])/model['std_value']\n",
    "                # Model support vectors, co-efficients & intercept\n",
    "                sup_vecs = np.array(model['support_vectors'])\n",
    "                n_support = np.array(model['n_support'])\n",
    "                dual_coefs = np.array(model['dual_coef'])\n",
    "                intercept = model['intercept']\n",
    "                n_featrs = model['n_features']\n",
    "                inp_var = model['input_variance']\n",
    "                gam_in = model['gamma']\n",
    "                gamma = (1/n_featrs) if gam_in == 'auto' else \\\n",
    "                ((1 / (n_featrs * inp_var)) if gam_in == 'scale' else float(gam_in))\n",
    "                if model['kernel'] == 'linear':\n",
    "                    kernal_out = np.dot(sup_vecs, x_input_sample.reshape(1, -1).T)\n",
    "                elif model['kernel'] == 'rbf':\n",
    "                    diff = sup_vecs - x_input_sample\n",
    "                    sup_vecs_len = np.shape(sup_vecs)[0]\n",
    "                    norm_val = np.array([np.linalg.norm(diff[n, :]) for n in range(sup_vecs_len)])\n",
    "                    kernal_out = np.exp(-gamma*(norm_val**2))\n",
    "                elif model['kernel'] == 'sigmoid':\n",
    "                    input_support = np.dot(sup_vecs, x_input_sample.reshape(1, -1).T)\n",
    "                    kernal_out = np.tanh(gamma * input_support + model['coef0'])\n",
    "                elif model['kernel'] == 'poly':\n",
    "                    input_support = np.dot(sup_vecs, x_input_sample.reshape(1, -1).T)\n",
    "                    kernal_out = (gamma * input_support + model['coef0'])**model['degree']\n",
    "                kernal_out = kernal_out.reshape(-1, 1)\n",
    "                decision_fn = svr_decision_function(kernal_out, n_support, dual_coefs, intercept, class_names)[0]\n",
    "                decision_fn = decision_fn.tolist()\n",
    "                # Obtain the class name where decision function value of the sample is max\n",
    "                sample_pred = class_names[decision_fn.index(max(decision_fn))]\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    # RF Model\n",
    "    elif model['model_type'] == 'RF':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - 0)/1\n",
    "                # Final prediction would be the class where sum of all trees probability is max.\n",
    "                trees = [t for t in model.keys() if 'tree_' in t]\n",
    "                sample_trees_number = ([tree_predict_list(x_input_sample, model[t]) for t in trees])\n",
    "                sample_trees_proba = [[x / sum(tr) for x in tr] for tr in sample_trees_number]\n",
    "                sample_proba = [sum(x) / len(x) for x in zip(*sample_trees_proba)]\n",
    "                # print('Predict_proba: ',sample_proba)\n",
    "                \n",
    "                # Obtain the class name where probability is max\n",
    "                sample_pred = class_names[sample_proba.index(max(sample_proba))]\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    # GradientBoost Model\n",
    "    elif model['model_type'] == 'GradientBoost':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - 0)/1\n",
    "                base_value = np.array(model['base_estimator_value'])\n",
    "                learning_rate = model['learning_rate']\n",
    "                # Final log probability would be sum of base estimator value and\n",
    "                # -(learning rate * tree's prediction)\n",
    "                trees = [t for t in model.keys() if 'tree_' in t]\n",
    "                if n_classes > 2:\n",
    "                    sample_pred_residual = [[tree_predict_list(x_input_sample, model[t][p]) for p in range(n_classes)] for t in trees]\n",
    "                else:\n",
    "                    sample_pred_residual = [[tree_predict_list(x_input_sample, model[t][0])] for t in trees]\n",
    "                \n",
    "                sample_pred_residual = np.array(sample_pred_residual)\n",
    "                sample_pred_update = np.sum([-(learning_rate * x) for x in sample_pred_residual], axis = 0)\n",
    "                sample_pred_fin = np.array(sample_pred_update).flatten()\n",
    "                sample_pred_fin = base_value - sample_pred_fin\n",
    "\n",
    "                # Final probability\n",
    "                if n_classes > 2:\n",
    "                    sample_pred_proba = np.exp(sample_pred_fin) / np.sum(np.exp(sample_pred_fin))\n",
    "                else:\n",
    "                    sample_pred_proba = np.exp(sample_pred_fin) / (1 + np.exp(sample_pred_fin))\n",
    "                    sample_pred_proba = np.array([1-sample_pred_proba[0], sample_pred_proba[0]])\n",
    "                #print('Predict_proba: ',sample_pred_proba)\n",
    "                # Final prediction\n",
    "                # Obtain the class name where probability is max\n",
    "                sample_pred = class_names[np.argmax(sample_pred_proba)]\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    # AdaBoost Model\n",
    "    elif model['model_type'] == 'AdaBoost':\n",
    "        for sample_i in range(x_input.shape[0]):\n",
    "            try:\n",
    "                # Preprocess data: Standardization\n",
    "                x_input_sample = (x_input[sample_i] - 0)/1\n",
    "                weights = np.array(model['estimator_weights'])\n",
    "\n",
    "                # Final prediction would be the class where sum of all trees probability is max.\n",
    "                trees = [t for t in model.keys() if 'tree_' in t]\n",
    "                \n",
    "                sample_trees_number = ([tree_predict_list(x_input_sample, model[t]) for t in trees])\n",
    "                sample_trees_proba = [[x / sum(tr) for x in tr] for tr in sample_trees_number]\n",
    "                \n",
    "                if model['algorithm'] == 'SAMME':\n",
    "                    # 'SAMME' uses sum of weights for each prediction from estimators. Thus each class will have\n",
    "                    # some total weights as the decision function\n",
    "                    \n",
    "                    sample_trees_class = [class_names[x.index(max(x))] for x in sample_trees_proba]\n",
    "                    sample_trees_class_bool = [ x == np.array(class_names) for x in sample_trees_class]\n",
    "                    # Decision function calculation\n",
    "                    sample_trees_decision = np.array([ x * weights[i] for i, x in enumerate(sample_trees_class_bool)])\n",
    "                elif model['algorithm'] == 'SAMME.R':\n",
    "                    # Weigts will be 1 for all estimators in 'SAMME.R'\n",
    "                    # 'SAMME.R' uses log proba for each prediction from estimators.\n",
    "                    \n",
    "                    # Clip probability from 0 to small value before taking log\n",
    "                    sample_trees_proba = [[max(x, 2.22044605e-16) for x in sublist] for sublist in sample_trees_proba]\n",
    "                    sample_trees_log_proba = np.log(sample_trees_proba)\n",
    "                    # Decision function calculation\n",
    "                    sample_trees_decision = np.array([[(n_classes - 1) * (x - (1/n_classes) * sum(xx)) for x in xx] for xx in sample_trees_log_proba])\n",
    "                    \n",
    "                sample_trees_decision = sum(sample_trees_decision)/sum(weights)\n",
    "                sample_trees_decision /= (n_classes - 1)\n",
    "                # Probability from Decision\n",
    "                ### Softmax\n",
    "                sample_trees_softmax = [np.exp(x) for x in sample_trees_decision] \n",
    "                sample_trees_softmax = [x/sum(sample_trees_softmax) for x in sample_trees_softmax] \n",
    "                sample_proba = sample_trees_softmax.copy()\n",
    "                #print('Predict_proba: ',sample_proba)\n",
    "                # Obtain the class name where probability is max\n",
    "                sample_pred = class_names[sample_proba.index(max(sample_proba))]\n",
    "            except Exception as error:\n",
    "                sample_pred = np.nan\n",
    "                print(error, file=sys.stderr)\n",
    "            output_list.append(sample_pred)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [1 2 1 2 0 0 1 4 2 0]\n",
      "Custom prediction:  ['1', '2', '1', '2', '0', '0', '1', '4', '2', '0']\n"
     ]
    }
   ],
   "source": [
    "# SVR testing\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = [1, 2, 1, 2, 0, 0, 1, 4, 2, 0]\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = SVC(kernel='rbf', degree=3, decision_function_shape = 'ovr',\n",
    "           gamma=3, coef0=0.0, \n",
    "           tol=0.001, C=5.0)\n",
    "model_SVR_pl = Pipeline([('standardize',StandardScaler()),('svr',algo)])\n",
    "model_SVR_sk = model_SVR_pl.fit(X, y)\n",
    "print('Sklearn prediction: ', model_SVR_sk.predict(X))\n",
    "#print('Sklearn decision_function: \\n', model_SVR_sk[1].decision_function(model_SVR_pl[0].transform(X)))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_svr', model_SVR_sk, X, y,'SVC')\n",
    "test_model_svr = decrypt_json('test_model_svr')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_svr, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [1 0 1 0 3 0 1 1 0 2]\n",
      "Custom prediction:  ['1', '0', '1', '0', '3', '0', '1', '1', '0', '2']\n"
     ]
    }
   ],
   "source": [
    "# RF testing\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = [1, 0, 1, 0, 3, 0, 1, 1, 0, 2]\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = RandomForestClassifier(n_estimators = 50, \n",
    "                             random_state=0, n_jobs=-1)\n",
    "model_RF_sk = algo.fit(X, y)\n",
    "print('Sklearn prediction: ', model_RF_sk.predict(X))\n",
    "#print('Sklearn predict_proba: ', model_RF_sk.predict_proba(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_rf', model_RF_sk, X, y,'RF')\n",
    "test_model_rf = decrypt_json('test_model_rf')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_rf, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [2 3 2 0 0 2 2 0 1 2 2 1]\n",
      "Custom prediction:  ['2', '3', '2', '0', '0', '2', '2', '0', '1', '2', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "# GradientBoost testing\n",
    "n_samples, n_features = 12, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = [2, 3, 2, 0, 0, 2, 2, 0, 1, 2, 2, 1]\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = GradientBoostingClassifier(n_estimators = 10, \n",
    "                                 learning_rate = 0.1, random_state=0)\n",
    "model_GB_sk = algo.fit(X, y)\n",
    "print('Sklearn prediction: ', model_GB_sk.predict(X))\n",
    "#print('Sklearn predict_proba: ', model_GB_sk.predict_proba(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_xgb', model_GB_sk, X, y,'GradientBoost')\n",
    "test_model_xgb = decrypt_json('test_model_xgb')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_xgb, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn prediction:  [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      "Custom prediction:  ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost testing\n",
    "n_samples, n_features = 12, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "# Model Training\n",
    "algo = AdaBoostClassifier(n_estimators = 10, algorithm = 'SAMME',\n",
    "                         learning_rate = 0.1, random_state=0)\n",
    "model_AGB_sk = algo.fit(X, y)\n",
    "print('Sklearn prediction: ', model_AGB_sk.predict(X))\n",
    "#print('Sklearn predict_proba: ', model_AGB_sk.predict_proba(X))\n",
    "\n",
    "folder_path = \"saved_models\\\\\"\n",
    "save_model_json(folder_path, 'test_model_agb', model_AGB_sk, X, y,'AdaBoost')\n",
    "test_model_xgb = decrypt_json('test_model_agb')\n",
    "print('Custom prediction: ', custom_predict_json(test_model_xgb, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
